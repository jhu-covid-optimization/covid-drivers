{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "morf-dev",
   "display_name": "morf-dev"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster time-series using DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from matplotlib import dates\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys; sys.path.append('../')\n",
    "from src.data_loader.data_loader import load_google_mobility, load_deaths, load_interventions, load_counties, load_google_mobility_time_series\n",
    "from src.utils.dates import get_today, lag_date, date2str, str2date, get_format\n",
    "from src.utils.df_utils import get_date_columns\n",
    "from src.pandas.align import align_lagged_dates\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Mobility Last Updated 04-26\nDeaths Last Updated 04-26\n"
    }
   ],
   "source": [
    "# Time series data\n",
    "mobility, mobility_date = load_google_mobility()\n",
    "deaths, deaths_date = load_deaths(join_county_codes=False)\n",
    "interventions, interventions_date = load_interventions()\n",
    "\n",
    "# Static data\n",
    "counties, counties_date = load_counties()\n",
    "\n",
    "# Processed mobility -> time series\n",
    "mobility_ts, mobility_ts_date = load_google_mobility_time_series()\n",
    "\n",
    "print(f'Mobility Last Updated {mobility_ts_date}')\n",
    "print(f'Deaths Last Updated {deaths_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore places with FIPS missing\n",
    "deaths = deaths.dropna(axis=0, subset=['FIPS']).astype({'FIPS':'int32'})\n",
    "deaths = deaths.merge(counties['FIPS'], on='FIPS', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns that are dates\n",
    "death_dates = get_date_columns(deaths, return_dtimes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(row):\n",
    "    FIPS = row['FIPS']\n",
    "    pop = counties[counties['FIPS'] == FIPS]['POP_ESTIMATE_2018']\n",
    "    return(row[death_dates] / int(pop))\n",
    "    \n",
    "def get_onset_date(row, thresh = 5):\n",
    "    above = row[row > thresh]\n",
    "    if len(above) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return above.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize deaths by pop total\n",
    "deaths = deaths[['FIPS']+death_dates]\n",
    "\n",
    "deaths['onset'] = deaths[death_dates].apply(lambda row: get_onset_date(row), axis=1)\n",
    "\n",
    "deaths = deaths.dropna(axis=1, subset=['onset'])\n",
    "\n",
    "deaths[death_dates] = deaths.apply(lambda row: normalize(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_df = deaths[deaths['onset'].apply(lambda d: (str2date(death_dates[-1]) - str2date(d)).days >= 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTWDistance(s1, s2,w=5):\n",
    "    s1 = s1[s1 >= 0]\n",
    "    s2 = s2[s2 >= 0]\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\t\t\n",
    "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "deaths_np = deaths_df.apply(lambda row: [row[d] for d in death_dates if (str2date(row['onset']) - str2date(d)).days <= n-1],axis=1)\n",
    "\n",
    "deaths_np = [moving_average(d, n=n) for d in deaths_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(d) for d in deaths_np])\n",
    "distmat = pairwise_distances([np.hstack((d, [-1]*(max_length - len(d)))) for d in deaths_np], metric=DTWDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/intermediates/dtw_distmat.pickle', distmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3: 0.5450836737669922\n4: 0.5791789321626356\n5: 0.6282699246597335\n6: 0.6402778782923592\n7: 0.6265909873390871\n8: 0.4822592942842327\n9: 0.4783608323833335\n"
    }
   ],
   "source": [
    "n_clusters = np.arange(3,10,1)\n",
    "#linkage = ['average', 'ward', '']\n",
    "for n_clusters in np.arange(3,10,1):\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', affinity='precomputed').fit(distmat)\n",
    "    score = silhouette_score(distmat, clustering.labels_, metric='precomputed')\n",
    "    print(f'{n_clusters}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(labels, label, n = 5):\n",
    "    idxs = np.where(labels == label)[0][:n]\n",
    "    n = min(len(idxs), n)\n",
    "    fig,axes = plt.subplots(n,1,figsize=(1.5*n,1.5*n), sharex=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i,idx in enumerate(idxs):\n",
    "        y = deaths_np[idx]\n",
    "        FIPS = deaths_df.iloc[idx]['FIPS']\n",
    "        l = ', '.join(counties[counties['FIPS'] == FIPS][['Area_Name', 'State']].values[0])\n",
    "        axes[i].plot(np.arange(len(y)), y, label=l)\n",
    "        axes[i].legend(loc='upper left')\n",
    "    axes[0].set_title(f'Cluster {label}, Size={len(np.where(labels == label)[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in clustering.labels_:\n",
    "    plot(clustering.labels_, label=label, n=5)"
   ]
  }
 ]
}